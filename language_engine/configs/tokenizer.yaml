# Preprocessing
normalizer:
  force_lowercase: True
  strip_accents: True
  force_english_keyboard: False
  whitespace_escape: False
tokenizer: SentencePieceBPE
vocab_size: 32768

# Dataset Formation
dataset_name: /home/penguin/GeorgianWritingWizard/data/whole_corpus/filter_v2/filtered.txt
seq_length: 512
include_cls_token_in_corpus: False
include_sep_token_in_corpus: True
use_type_ids: False
name: geo_tokenizer