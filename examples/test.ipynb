{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c150b370a9470bade3592a7a2c046c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_geo = AutoTokenizer.from_pretrained('ZurabDz/albert-geo')\n",
    "tokenizer_mix = AutoTokenizer.from_pretrained('/home/penguin/GeorgianWritingWizard/language_engine/examples/ff_tok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('geo/tokenizer_config.json',\n",
       " 'geo/special_tokens_map.json',\n",
       " 'geo/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_geo.save_pretrained('geo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [6361, 4286, 4277, 12990, 4], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_geo('გამარჯობა როგორ ხარ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 8, 1, 8, 1, 8, 1, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_mix('გამარჯობა როგორ ხარ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration ZurabDz--geo_small_corpus-297bec1b2e1a3b51\n",
      "Found cached dataset parquet (/home/penguin/.cache/huggingface/datasets/ZurabDz___parquet/ZurabDz--geo_small_corpus-297bec1b2e1a3b51/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce45a45d09e439f910f2c271455c56f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('ZurabDz/albert-geo')\n",
    "dataset = load_dataset('ZurabDz/geo_small_corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 7375931\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_max_length = 1e30\n",
    "\n",
    "def filtering_rule(examples):\n",
    "    tokenized = tokenizer(examples['text'])[\"input_ids\"]\n",
    "    return [len(t) < 0.3 * len(e) for t, e in zip(tokenized, examples['text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_setup = dict(\n",
    "    batched=True,\n",
    "    batch_size=1024,\n",
    "    num_proc=8\n",
    "        # load_from_cache_file=False,\n",
    "        # keep_in_memory=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset before trash removal: 7375931.\n",
      "              "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20dd50ed5eaa44888a6136daaf09da71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter sentences that cannot be tokenized well. #6:   0%|          | 0/901 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c948a20065b42e0bbcff67675a86f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter sentences that cannot be tokenized well. #2:   0%|          | 0/901 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff924269f8d422cb61310579cee5d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter sentences that cannot be tokenized well. #7:   0%|          | 0/901 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7988f8c6e26943ac97754b880f4df40a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter sentences that cannot be tokenized well. #4:   0%|          | 0/901 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f19690c6f4f406991c15dfae4cfbebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter sentences that cannot be tokenized well. #3:   0%|          | 0/901 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a2eaa698b34727b75469ad813c53c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter sentences that cannot be tokenized well. #5:   0%|          | 0/901 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f56daa417c445c5bb9a9e391e8c34f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter sentences that cannot be tokenized well. #1:   0%|          | 0/901 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a34cea4c2343769966c055f3f278f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter sentences that cannot be tokenized well. #0:   0%|          | 0/901 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of filtered dataset: 6185382.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of dataset before trash removal: {len(raw_dataset)}.\")\n",
    "raw_dataset = raw_dataset.filter(\n",
    "    filtering_rule,\n",
    "    desc=\"Filter sentences that cannot be tokenized well.\",\n",
    "    **map_setup,\n",
    "    # keep_in_memory=True,  # can run out of mem even on the 750GB node?\n",
    ")\n",
    "print(f\"Size of filtered dataset: {len(raw_dataset)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ba0b4d16a94676a5a523dec89db63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/6186 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6edd24d9a6f47b782fcd07a38f3a220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/8 shards):   0%|          | 0/6185382 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_dataset.save_to_disk('filtered_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tok = AutoTokenizer.from_pretrained('albert-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m new_tok \u001b[39m=\u001b[39m eng_tok\u001b[39m.\u001b[39;49mtrain_new_from_iterator(raw_dataset, vocab_size\u001b[39m=\u001b[39;49m\u001b[39m30_000\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:709\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.train_new_from_iterator\u001b[0;34m(self, text_iterator, vocab_size, length, new_special_tokens, special_tokens_map, **kwargs)\u001b[0m\n\u001b[1;32m    707\u001b[0m trainer_class \u001b[39m=\u001b[39m MODEL_TO_TRAINER_MAPPING[tokenizer_json[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[1;32m    708\u001b[0m trainer \u001b[39m=\u001b[39m trainer_class(vocab_size\u001b[39m=\u001b[39mvocab_size, special_tokens\u001b[39m=\u001b[39mspecial_tokens, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 709\u001b[0m tokenizer\u001b[39m.\u001b[39;49mtrain_from_iterator(text_iterator, length\u001b[39m=\u001b[39;49mlength, trainer\u001b[39m=\u001b[39;49mtrainer)\n\u001b[1;32m    711\u001b[0m \u001b[39mif\u001b[39;00m post_processor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    712\u001b[0m     trained_tokenizer_json \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(tokenizer\u001b[39m.\u001b[39mto_str())\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.8/site-packages/datasets/arrow_dataset.py:2195\u001b[0m, in \u001b[0;36mDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2193\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2194\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_rows):\n\u001b[0;32m-> 2195\u001b[0m         \u001b[39myield\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem(\n\u001b[1;32m   2196\u001b[0m             i,\n\u001b[1;32m   2197\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.8/site-packages/datasets/arrow_dataset.py:2585\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2583\u001b[0m format_kwargs \u001b[39m=\u001b[39m format_kwargs \u001b[39mif\u001b[39;00m format_kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m   2584\u001b[0m formatter \u001b[39m=\u001b[39m get_formatter(format_type, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mformat_kwargs)\n\u001b[0;32m-> 2585\u001b[0m pa_subtable \u001b[39m=\u001b[39m query_table(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data, key, indices\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_indices \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_indices \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m   2586\u001b[0m formatted_output \u001b[39m=\u001b[39m format_table(\n\u001b[1;32m   2587\u001b[0m     pa_subtable, key, formatter\u001b[39m=\u001b[39mformatter, format_columns\u001b[39m=\u001b[39mformat_columns, output_all_columns\u001b[39m=\u001b[39moutput_all_columns\n\u001b[1;32m   2588\u001b[0m )\n\u001b[1;32m   2589\u001b[0m \u001b[39mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.8/site-packages/datasets/formatting/formatting.py:587\u001b[0m, in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    585\u001b[0m     _check_valid_column_key(key, table\u001b[39m.\u001b[39mcolumn_names)\n\u001b[1;32m    586\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 587\u001b[0m     size \u001b[39m=\u001b[39m indices\u001b[39m.\u001b[39;49mnum_rows \u001b[39mif\u001b[39;00m indices \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m table\u001b[39m.\u001b[39mnum_rows\n\u001b[1;32m    588\u001b[0m     _check_valid_index_key(key, size)\n\u001b[1;32m    589\u001b[0m \u001b[39m# Query the main table\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/research/lib/python3.8/site-packages/datasets/table.py:444\u001b[0m, in \u001b[0;36mTable.num_rows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    434\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnum_rows\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    435\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[39m    Number of rows in this table.\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[39m        int\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 444\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtable\u001b[39m.\u001b[39;49mnum_rows\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "new_tok = eng_tok.train_new_from_iterator(raw_dataset, vocab_size=30_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31bef8f0e01d86822998708cae530d3f279653869977ee7676ea73d628f47b0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
